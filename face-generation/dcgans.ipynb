{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "img_dir=\"C:/Desktop/Deep learning/project/img_align_celeba/\"\n",
    "z_dim=100\n",
    "batch_size=32\n",
    "gen_size1=4\n",
    "gen_filter1=512\n",
    "gen_filter2=gen_filter1//2\n",
    "gen_filter3=gen_filter2//2\n",
    "gen_filter4=gen_filter3//2\n",
    "img_filter=3\n",
    "\n",
    "dis_filter1=64\n",
    "dis_filter2=dis_filter1*2\n",
    "dis_filter3=dis_filter2*2\n",
    "dis_filter4=dis_filter3*2\n",
    "\n",
    "img_size=64\n",
    "stddev=0.02  \n",
    "learning_rate=0.0002\n",
    "epochs=1\n",
    "restore=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_resize(image_path, batch_num,resize_shape=(img_size,img_size)):\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width, channel = image.shape\n",
    "\n",
    "    if width == height:\n",
    "        resized_image = cv2.resize(image, resize_shape)\n",
    "    elif width > height:\n",
    "        resized_image = cv2.resize(image, (int(width * float(resize_shape[0])/height), resize_shape[1]))\n",
    "        cropping_length = int( (resized_image.shape[1] - resize_shape[0]) / 2)\n",
    "        resized_image = resized_image[:,cropping_length:cropping_length+resize_shape[1]]\n",
    "    else:\n",
    "        resized_image = cv2.resize(image, (resize_shape[0], int(height * float(resize_shape[1])/width)))\n",
    "        cropping_length = int( (resized_image.shape[0] - resize_shape[1]) / 2)\n",
    "        resized_image = resized_image[cropping_length:cropping_length+resize_shape[0], :]\n",
    "\n",
    "    return resized_image/127.5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load batch of images\n",
    "img_name= list(filter(lambda x: x.endswith(\".jpg\"),os.listdir(img_dir)))\n",
    "np.random.shuffle(img_name)\n",
    "\n",
    "\n",
    "def load_img(batch_size,batch_num):\n",
    "    imgs=[]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image_path=img_dir+str(img_name[batch_size*batch_num+i])\n",
    "        img=crop_resize(image_path, batch_num,resize_shape=(img_size,img_size))\n",
    "        imgs.append(img)\n",
    "    \n",
    "    return np.array(imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_img=tf.placeholder(dtype=tf.float32,shape=(None,img_size,img_size,3))\n",
    "input_Z=tf.placeholder(dtype=tf.float32,shape=(None,100))\n",
    "training= tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x,alpha=0.2):\n",
    "    return tf.maximum(0.2*x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv_2d(input_,filter_,kern_size,stride,activation_=None,pad=\"same\",stddev=0.02):\n",
    "    \n",
    "    deconv=tf.layers.conv2d_transpose(inputs=input_,filters=filter_,kernel_size=kern_size,\n",
    "                                      strides=stride,padding=pad,\n",
    "                                      kernel_initializer=tf.random_normal_initializer(stddev=stddev),\n",
    "                                      activation=activation_)\n",
    "    \n",
    "    return deconv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator\n",
    "def generator(x,train=True):\n",
    "    with tf.variable_scope(\"generator\", reuse=False if train==True else True):\n",
    "        flat_layer1=tf.layers.dense(x,(gen_size1*gen_size1*gen_filter1),\n",
    "                                    kernel_initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "        L1=tf.reshape(flat_layer1,shape=[batch_size,gen_size1,gen_size1,gen_filter1])\n",
    "        L1=tf.nn.relu(L1)\n",
    "        \n",
    "        L2=deconv_2d(L1,gen_filter2,5,2,tf.nn.relu,pad=\"same\",stddev=stddev)\n",
    "        bn2=tf.layers.batch_normalization(L2,training=train)\n",
    "        \n",
    "        L3=deconv_2d(bn2,gen_filter3,5,2,tf.nn.relu,pad=\"same\",stddev=stddev)\n",
    "        bn3=tf.layers.batch_normalization(L3,training=train)\n",
    "        \n",
    "        L4=deconv_2d(bn3,gen_filter4,5,2,tf.nn.relu,pad=\"same\",stddev=stddev)\n",
    "        bn4=tf.layers.batch_normalization(L4,training=train)\n",
    "        \n",
    "        L5=deconv_2d(bn4,img_filter,5,2,None,pad=\"same\",stddev=stddev)\n",
    "        \n",
    "        \n",
    "        L5=tf.nn.tanh(L5)\n",
    "\n",
    "        \n",
    "    return L5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input_,filter_,kern_size,stride,activation_,pad=\"same\",stddev=0.02):\n",
    "    L1=tf.layers.conv2d(inputs=input_,filters=filter_,kernel_size=kern_size,strides=stride\n",
    "                        ,activation=activation_,padding=pad,\n",
    "                        kernel_initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "    return L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminate(input_,train,reuse=False):\n",
    "    with tf.variable_scope(\"disc\", reuse=reuse):\n",
    "        L1=conv2d(input_,dis_filter1,5,2,lrelu,pad=\"same\",stddev=0.02)\n",
    "        bn1=tf.layers.batch_normalization(L1,training=train)\n",
    "        \n",
    "        L2=conv2d(bn1,dis_filter2,5,2,lrelu,pad=\"same\",stddev=0.02)\n",
    "        bn2=tf.layers.batch_normalization(L2,training=train)\n",
    "        \n",
    "        L3=conv2d(bn2,dis_filter3,5,2,lrelu,pad=\"same\",stddev=0.02)\n",
    "        bn3=tf.layers.batch_normalization(L3,training=train)\n",
    "        \n",
    "        L4=conv2d(bn3,dis_filter4,5,2,lrelu,pad=\"same\",stddev=0.02)\n",
    "        bn4=tf.layers.batch_normalization(L4,training=train)\n",
    "        \n",
    "        L5=tf.reshape(bn4,shape=[batch_size,4*4*dis_filter4])\n",
    "        logit=tf.layers.dense(inputs=L5,units=1,kernel_initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "        out=tf.nn.sigmoid(logit)\n",
    "        \n",
    "    return out, logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcgans(img,z,train=True):\n",
    "    img_fake=generator(input_Z,train=True)\n",
    "    out_disc_fake,logit_disc_fake=discriminate(img_fake,train=train,reuse=False)\n",
    "    out_disc_real,logit_disc_real=discriminate(img,train=train,reuse=True)\n",
    "    \n",
    "    d_loss_fake= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(logit_disc_fake),logits=logit_disc_fake))\n",
    "    d_loss_real= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(logit_disc_real),logits=logit_disc_real))\n",
    "    g_loss= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(logit_disc_fake),logits=logit_disc_fake))\n",
    "    \n",
    "    \n",
    "    d_loss_sum=d_loss_fake+d_loss_real\n",
    "    \n",
    "    \n",
    "    t_vars=tf.trainable_variables()\n",
    "    d_vars=[var for var in t_vars if var.name.startswith(\"disc\")]\n",
    "    g_vars=[var for var in t_vars if var.name.startswith(\"generator\")]\n",
    "    \n",
    "    \n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): \n",
    "        g_train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(g_loss,var_list=g_vars)\n",
    "        d_train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(d_loss_sum,var_list=d_vars)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return g_train,d_train,g_loss,d_loss_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train,d_train,g_loss,d_loss_sum=dcgans(real_img,input_Z,training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "sess=tf.Session()\n",
    "\n",
    "if (restore==True):\n",
    "    saver.restore(sess, \"./model\")\n",
    "    print(\"Model restored.\")\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator_loss:0.04320420324802399\n",
      "generator_loss:7.598213195800781\n",
      "Model saved in path: ./model-0\n",
      "discriminator_loss:0.022603442892432213\n",
      "generator_loss:6.488251686096191\n",
      "discriminator_loss:0.013235383667051792\n",
      "generator_loss:11.831184387207031\n",
      "discriminator_loss:0.020733986049890518\n",
      "generator_loss:10.020547866821289\n",
      "discriminator_loss:0.05553687736392021\n",
      "generator_loss:7.445965766906738\n",
      "Model saved in path: ./model-400\n",
      "discriminator_loss:0.06603589653968811\n",
      "generator_loss:10.39197063446045\n",
      "discriminator_loss:0.0953136458992958\n",
      "generator_loss:6.193470001220703\n",
      "discriminator_loss:0.7188040018081665\n",
      "generator_loss:8.784385681152344\n",
      "discriminator_loss:0.5288658738136292\n",
      "generator_loss:6.330685615539551\n",
      "Model saved in path: ./model-800\n",
      "discriminator_loss:0.03925750404596329\n",
      "generator_loss:7.580720901489258\n",
      "discriminator_loss:0.07430591434240341\n",
      "generator_loss:8.830081939697266\n",
      "discriminator_loss:0.2054368257522583\n",
      "generator_loss:3.5655436515808105\n",
      "discriminator_loss:0.0710497796535492\n",
      "generator_loss:7.202668190002441\n",
      "Model saved in path: ./model-1200\n",
      "discriminator_loss:0.734900951385498\n",
      "generator_loss:5.151408672332764\n",
      "discriminator_loss:0.1751251220703125\n",
      "generator_loss:4.56168270111084\n",
      "discriminator_loss:0.054914359003305435\n",
      "generator_loss:8.487279891967773\n",
      "discriminator_loss:0.039774950593709946\n",
      "generator_loss:6.748053550720215\n",
      "Model saved in path: ./model-1600\n",
      "discriminator_loss:0.34930843114852905\n",
      "generator_loss:5.46935510635376\n",
      "discriminator_loss:0.02304605022072792\n",
      "generator_loss:8.326828002929688\n",
      "discriminator_loss:0.054781872779130936\n",
      "generator_loss:6.933433532714844\n",
      "discriminator_loss:0.1920498013496399\n",
      "generator_loss:6.428179740905762\n",
      "Model saved in path: ./model-2000\n",
      "discriminator_loss:0.03568534553050995\n",
      "generator_loss:9.269916534423828\n",
      "discriminator_loss:0.18866755068302155\n",
      "generator_loss:3.575495481491089\n",
      "discriminator_loss:0.13920173048973083\n",
      "generator_loss:6.590719699859619\n",
      "discriminator_loss:0.1520095020532608\n",
      "generator_loss:11.770586013793945\n",
      "Model saved in path: ./model-2400\n",
      "discriminator_loss:0.09277675300836563\n",
      "generator_loss:6.142810821533203\n",
      "discriminator_loss:0.03242646902799606\n",
      "generator_loss:7.424288749694824\n",
      "discriminator_loss:0.01426625344902277\n",
      "generator_loss:6.706793785095215\n",
      "discriminator_loss:0.06828130781650543\n",
      "generator_loss:8.680601119995117\n",
      "Model saved in path: ./model-2800\n",
      "discriminator_loss:0.14874501526355743\n",
      "generator_loss:5.525306224822998\n",
      "discriminator_loss:0.22641947865486145\n",
      "generator_loss:5.583952903747559\n",
      "discriminator_loss:0.12540268898010254\n",
      "generator_loss:5.300516128540039\n",
      "discriminator_loss:0.04592704772949219\n",
      "generator_loss:5.289848327636719\n",
      "Model saved in path: ./model-3200\n",
      "discriminator_loss:0.036175262182950974\n",
      "generator_loss:6.410199165344238\n",
      "discriminator_loss:0.07241500169038773\n",
      "generator_loss:6.705028533935547\n",
      "discriminator_loss:0.011712086386978626\n",
      "generator_loss:6.85465145111084\n",
      "discriminator_loss:0.31940972805023193\n",
      "generator_loss:7.616004943847656\n",
      "Model saved in path: ./model-3600\n",
      "discriminator_loss:0.13509376347064972\n",
      "generator_loss:4.902515411376953\n",
      "discriminator_loss:0.0436878502368927\n",
      "generator_loss:3.7786149978637695\n",
      "discriminator_loss:0.0854182243347168\n",
      "generator_loss:5.99957275390625\n",
      "discriminator_loss:0.06566654145717621\n",
      "generator_loss:5.158996105194092\n",
      "Model saved in path: ./model-4000\n",
      "discriminator_loss:0.01571970246732235\n",
      "generator_loss:5.756180763244629\n",
      "discriminator_loss:0.07161502540111542\n",
      "generator_loss:11.025179862976074\n",
      "discriminator_loss:0.010042167268693447\n",
      "generator_loss:8.953544616699219\n",
      "discriminator_loss:0.011269458569586277\n",
      "generator_loss:8.188057899475098\n",
      "Model saved in path: ./model-4400\n",
      "discriminator_loss:0.03624262660741806\n",
      "generator_loss:8.866975784301758\n",
      "discriminator_loss:0.08752904832363129\n",
      "generator_loss:6.750658988952637\n",
      "discriminator_loss:0.06783824414014816\n",
      "generator_loss:5.087885856628418\n",
      "discriminator_loss:0.4181218445301056\n",
      "generator_loss:5.214644432067871\n",
      "Model saved in path: ./model-4800\n",
      "discriminator_loss:0.0018733335891738534\n",
      "generator_loss:9.743099212646484\n",
      "discriminator_loss:0.030982214957475662\n",
      "generator_loss:5.712136268615723\n",
      "discriminator_loss:0.29020848870277405\n",
      "generator_loss:5.242618560791016\n",
      "discriminator_loss:0.0675717368721962\n",
      "generator_loss:6.245283126831055\n",
      "Model saved in path: ./model-5200\n",
      "discriminator_loss:0.05399714782834053\n",
      "generator_loss:8.666230201721191\n",
      "discriminator_loss:0.025743400678038597\n",
      "generator_loss:7.063597679138184\n",
      "discriminator_loss:0.14072836935520172\n",
      "generator_loss:10.04640007019043\n",
      "discriminator_loss:0.05637558922171593\n",
      "generator_loss:6.742219924926758\n",
      "Model saved in path: ./model-5600\n",
      "discriminator_loss:0.07462950795888901\n",
      "generator_loss:5.594866752624512\n",
      "discriminator_loss:0.24755604565143585\n",
      "generator_loss:16.76894760131836\n",
      "discriminator_loss:0.11119314283132553\n",
      "generator_loss:9.812747955322266\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(len(img_name)//batch_size):\n",
    "        Z=np.random.uniform(-1, 1, size=[batch_size, z_dim]).astype(np.float32)\n",
    "        img_batch=load_img(batch_size,i)\n",
    "        \n",
    "        with tf.device('/gpu:0'):\n",
    "            _,dloss=sess.run([d_train,d_loss_sum],feed_dict={real_img:img_batch,input_Z:Z,training:True})\n",
    "            _,gloss=sess.run([g_train,g_loss],feed_dict={real_img:img_batch,input_Z:Z,training:True})\n",
    "    \n",
    "\n",
    "        if (i%100==0):\n",
    "            print(\"discriminator_loss:{}\".format(dloss))\n",
    "            print(\"generator_loss:{}\".format(gloss))\n",
    "        \n",
    "        if(i%400==0):\n",
    "            save_path = saver.save(sess, \"./model\",global_step=i)\n",
    "            print(\"Model saved in path: %s\" % save_path)\n",
    "            a=sess.run(generator(input_Z,train=training),feed_dict={real_img:img_batch,input_Z:Z,training:False})\n",
    "            cv2.imwrite(\"C:/Deep learning/project/face/vis/\"+str(i)+\".png\",(a[0,...]+1.)*127.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
